---
title: "Biostat 203B Homework 4"
subtitle: Due Mar 18 @ 11:59PM
output:
  html_document:
    theme: spacelab
    highlight: textmate
    toc: true
    toc_float: true
    number_sections: false
    fig_width: 10 
    fig_height: 10 
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```


```{r, include= FALSE}
## Color Format
colFmt <- function(x,color) {
  
  outputFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
  
  if(outputFormat == 'latex') {
    ret <- paste("\\textcolor{",color,"}{",x,"}",sep="")
  } else if(outputFormat == 'html') {
    ret <- paste("<font color='",color,"'>",x,"</font>",sep="")
  } else {
    ret <- x
  }
  return(ret)
}
```

Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(miceRanger))
library(knitr)
```






## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 

### 1. Explain the jargon MCAR, MAR, and MNAR.

`r colFmt("Solution: MCAR, MAR, and MNAR are different methods of classifying missing data. MCAR stands for Missing Completely at random.  MCAR assumes that whether a datum is missing is independent of the data itself, and that the probability of a missing 
value is the same for all values. MAR stands for Missing at Random. NMAR stands for Not Missing at Random, and is the third classification used when 
missingness is neither MCAR nor MAR. Missingness may be occuring because of some
unmeasured variable.",'blue4')`

### 2. Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

`r colFmt("The MICE algorithm works to impute missing data by computing many models to predict missing values. For each iteration, the missing value is filled in using
other values in the dataset. ", 'blue4')`


### 3. Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

```{r, results = 'asis'}
icu <- read_rds("~/biostat-203b-2022-winter/hw3/mimiciv_shiny/icu_cohort.rds") 
```

```{r}
icu$thirty_day_mort[is.na(icu$thirty_day_mort)] <- 0
```

```{r}
colSums(is.na(icu))
```
Determine outliers that may be due to missing data/data entry errors. 

```{r, results = 'asis'}
var <- integer()
low <- numeric()
high <- numeric()
outlier <- as.data.frame(cbind(var, low, high))
n <- 1
x <- c(20:34)
for (i in x){
  q1 <- quantile(icu[,i], .25, na.rm = TRUE)
  q3 <- quantile(icu[,i], .75, na.rm = TRUE)
  outlier[n,] <- c(i,  q1 - 1.5 * (q3 - q1), q3 + 1.5 * (q3 - q1))
  n = n + 1
}
kable(outlier)
```

Replace data that is above or below these values with NAs

```{r}
n <- 1
for (i in x){
 r <- which(icu[,i] > outlier[n,3] | icu[,i] < outlier[n,2])
 icu[r,i] <- NA
 n <- n +1
}
```

Now let's examine how many NAs there are

```{r}
colSums(is.na(icu))
```
Remove columns with more than 5000 NAs:

```{r}
icu <- icu %>% select( which(colSums(is.na(icu)) < 5000))
```

### 4. Impute missing values by `miceRanger` (request $m=3$ data sets). This step is computational intensive. Make sure to save the imputation results as a file. Hint: Setting `max.depth=10` in the `miceRanger` function may cut some computing time.

```{r, eval=FALSE}
mice <- miceRanger(icu, m = 3, returnModels = FALSE, max.depth = 10)
```
```{r, eval = FALSE}
  write_rds(mice,"mice_results.rds", compress = "gz")
```

```{r}
mice_res <- read_rds("mice_results.rds")
```
### 5. Make imputation diagnostic plots and explain what they mean. {.tabset}

#### Distribution of Imputed Values
```{r}
plotDistributions(mice_res,vars='allNumeric')
```

#### Convergence of Correlation
```{r}
plotCorrelations(mice_res,vars='allNumeric')
```

#### Center & Dispersion Convergence
```{r}
plotVarConvergence(mice_res,vars='allNumeric')
```

#### Model OOB Error
```{r}
plotModelError(mice_res,vars='allNumeric')
```

#### Variable Importance
```{r}
plotVarImportance(mice_res)
```

#### Imputed Variance Between Datasets
```{r}
plotImputationVariance(mice_res, ncol=2, widths=c(5,3))
```


### 6. Choose one of the imputed data sets to be used in Q2. This is **not** a good idea to use just one imputed data set or to average multiple imputed data sets. Explain in a couple of sentences what the correct Multiple Imputation strategy is.

```{r}
mice_data <- completeData(mice_res)
mice_data <- mice_data$Dataset_1
```

Change categorical variables into factors:
```{r}
mice_data <- mice_data %>% mutate(marital_status_fact = as.factor(marital_status), 
                     gender_fact = as.factor(gender), 
                     ethnicity_fact = as.factor(ethnicity), 
                     thirty_fact = as.factor(thirty_day_mort))
```


## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function in base R or keras), (2) logistic regression with lasso penalty (glmnet or keras package), (3) random forest (randomForest package), or (4) neural network (keras package).

### 1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.

We will use the package `splitTools` to create a partition of our data into 80% training set and 20% test set. 
```{r}
library(splitTools)
set.seed(3602)
#only keep necessary predictor and outcome variables

mice_split <- partition(mice_data$thirty_day_mort,
                        p = c(train = 0.8, test = 0.2))
test <- mice_data[mice_split$test, ]
train <- mice_data[mice_split$train, ]
```


### 2. Train the models using the training set. {.tabset}

#### Method 1: Logistic regression 

Select model using LASSO. Process guided by ISLR Ch 6.6 pages 251 - 255.
```{r}
library(glmnet)
preds <- model.matrix(thirty_fact~., train[,c(13:16, 18:32,38)])[,-1]
y <- as.numeric(train$thirty_fact)
lass <- glmnet(preds, y, alpha = 1)

preds_new <-  model.matrix(thirty_fact~., test[,c(13:16, 18:32,38)])[,-1]
y_new <- as.numeric(test$thirty_fact)

cv <- cv.glmnet(preds, y, alpha = 1)

x <- model.matrix(thirty_fact~., mice_data[,c(13:16, 18:32,38)])[,-1]
y <- as.numeric(mice_data$thirty_fact)
out <- glmnet(x, y,
              alpha = 1)
coeff <- predict(out, type = "coefficients", s= cv$lambda.1se)
coeff[which(round(coeff,3) != 0),]
```

From the LASSO selection, it appears that an indicator variable for ethnicity unable to obtain, ethnicity unknown, gender, age, and all lab/vital measurements except for meas_50983, meas_50971, meas_50931, and meas_220181




```{r}
mice_logistic <- glm(thirty_day_mort ~ gender + anchor_age  + 
                       as.numeric(ethnicity == "UNABLE TO OBTAIN") +
                       as.numeric(ethnicity == "UNKNOWN")+  meas_51221 +
                       meas_50882 + meas_50912 + meas_51301 + meas_50902 + 
                       meas_50960 + meas_220210 + meas_220045 + meas_223761 +  
                       meas_220179, data = train, family = binomial)
```


#### Method 2: Random Forests
```{r}
library(randomForest)

mice_forest <- randomForest(thirty_fact ~ gender_fact + anchor_age + marital_status_fact + 
                       ethnicity_fact +  meas_51221 + meas_50882 + meas_50912 + 
                       meas_51301 + meas_50902 + meas_50960 + meas_50983 + 
                       meas_50893 +  meas_50971 + meas_50931 + meas_220210 +
                       meas_220045 + meas_223761+ meas_220181 + 
                       meas_220179, data = train)
  
```




### 3. Compare model prediction performance on the test set. {.tabset}

We will use the AUC of the ROC curve to evaluate predictive performance. The package `pROC` can calculate the AUC and plot the ROC curves. 

```{r, eval = FALSE}
install.packages("pROC")
```
```{r}
library(pROC)
```


#### Method 1: Logistic Regression

```{r}
#Predicted values in the test data using logistic regression model.
yhat_logistic <- predict(mice_logistic, type = "response", newdata = test)
```

ROC and AUC: 
```{r}
roc_logistic <- roc(as.numeric(test$thirty_fact), yhat_logistic)

auc(roc_logistic)
plot(roc_logistic)
```

#### Method 2: Random Forests
```{r}
yhat_forest <- predict(mice_forest, type = "response", newdata = test)

```

```{r}
roc_logistic <- roc(as.numeric(test$thirty_fact), as.numeric(yhat_forest))

auc(roc_logistic)
plot(roc_logistic)
```

### Remarks
We see that the AUC for the logistic regression is substantially larger than that of the random forest. That is, the logistic model with variable selection using LASSO regression predicts 30-day mortality better than the random forest. There are a few pros and cons to each method which might explain the difference in performance between the two methods, and also might affect the interpretability and usefullness of the model. 
